{
 "metadata": {
  "name": "2_scrapy_it_gets_the_web"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Scrapy: It GETs the web"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Asheesh Laroia\n",
      "asheesh@asheesh.org"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Scraping is easy\n",
      "- download with urllib2, requests, mechanize\n",
      "- examine with browser inspectors\n",
      "- Parse with lxml/BeautifulSoup\n",
      "- Select with XPath or CSSSelector\n",
      "\n",
      "Rewriting some non-scrapy code\n",
      "- Get list of scrapers\n",
      "- Get web page, parse with lxml, use css selector\n",
      "- Easy to get speakers, getting titles is harder\n",
      "\n",
      "Capture titles\n",
      "- store_datum()\n",
      "- For each element pass into store_datum()\n",
      "\n",
      "Problem: store_datum() crashes\n",
      "- Buggy extraction and buggy data saving\n",
      "Solution: Scrapy Items() (like namedtuple + exception)\n",
      "\n",
      "So return items as list and then store_datum()\n",
      "\n",
      "Now you may want to change a lot of code:\n",
      "  CREATE SPIDERS\n",
      "  \n",
      "Run spider from CLI (only errors)\n",
      "`scrapy runspider your_spider.py -L ERROR`\n",
      "\n",
      "By default to json via stdout\n",
      "`scrapy runspider your_spider.py -L ERROR -s FEED_URI=myfile.out`\n",
      "\n",
      "Internet <-> Spider -> Parse -> Pipeline -> Feed Exporter -> JSON\n",
      "\n",
      "Can update pipeline with process_item() functions\n",
      "\n",
      "Scrapy wants you to make a project!\n",
      "\n",
      "Great feature: scrapyd\n",
      "Great feature: telnet localhost 6023\n",
      "scrapy runspider your_spider.py -s TELNETCONSOLE (turn off telnet for production)\n",
      "\n",
      "Somewhat complex for integration with other projects\n",
      "\n",
      "Scrapy is Async!\n",
      "\n",
      "If you're not done say so!\n",
      "\n",
      "Continue with another Request item (yield request)\n",
      "which feeds to a callback handler and yields an Item()\n",
      "To send data between use response.meta dict\n",
      "\n",
      "You get a lot of performance from this\n",
      "\n",
      "serial -> multiprocessing -> gevent -> scrapy\n",
      "\n",
      "Write tests!\n",
      "- Create a response object with HtmlResponse, body is a saved html\n",
      "- create spider and send response to parse() method\n",
      "- Multiple links use autoresponse.Autoresponder\n",
      "\n",
      "Tricks:\n",
      "- Setting for everything\n",
      "- Use spidermonkey for Javascript, grab scripts and run\n",
      "- Use selenium if spidermonkey doesn't work and connect to browser\n",
      "- DjangoItem() for django + process_item() in pipeline to call save()\n",
      "- Alternatively dump out json files and read them separately to put into Django\n",
      "\n",
      "Best Practice:\n",
      "- Leave HTTP to Scrapy\n",
      "- Impatient? Use the Item Pipeline\n",
      "- Patient? Use the Feed Exporter\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Personal:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Use Pipeline for saving/processing"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}